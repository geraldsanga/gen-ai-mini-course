{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "PaperAI, a simple Retrieval-Augmented Generation (RAG) python script to retrieve information from documents using LangChain.\n",
    "This script:\n",
    "  1. Splits a PDF document into manageable text chunks.\n",
    "  2. Embeds these chunks and stores them in a pgVector database.\n",
    "  3. Captures user queries and retrieves the most relevant context.\n",
    "  4. Answers the user’s question using a Large language model.\n",
    "  5. Uses Ollama to run Models locally"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b15e37944008b454"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Importing required modules"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e47f0855d096d6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.vectorstores.pgvector import PGVector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:05.292383Z",
     "start_time": "2025-02-11T15:10:05.288895Z"
    }
   },
   "id": "b62c453384ab2e23",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 1: Load and split the PDF ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83380dff38273ccf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from ./assets/all_salim_1.pdf…\n",
      "Loaded 15 document(s) from the PDF.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"./assets/all_salim_1.pdf\"  # Replace with your actual PDF file path\n",
    "print(f\"Loading PDF from {pdf_path}…\")\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} document(s) from the PDF.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:11.649984Z",
     "start_time": "2025-02-11T15:10:09.915975Z"
    }
   },
   "id": "209b4552929a5a1c",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted the document into 45 charming chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Splitted the document into {len(chunks)} charming chunks.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:12.826075Z",
     "start_time": "2025-02-11T15:10:12.816514Z"
    }
   },
   "id": "37523ecc9d01e41b",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### We will create simpler documents using a list of sentences as our internal knowledge"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b087f348165695c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "simpler_chunks = [\n",
    "    \"Generative AI is here and it's here to stay\",\n",
    "    \"The rise of Generative AI can be compared to the internet bubble\",\n",
    "    \"Our Gen AI company is called Futuristic Phenomena\"\n",
    "]\n",
    "\n",
    "documents = [Document(page_content=chunk, metadata={\"source\": \"generated\"}) for chunk in simpler_chunks]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:17.703671Z",
     "start_time": "2025-02-11T15:10:17.694775Z"
    }
   },
   "id": "8770e7e5b3d04805",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 2: Embed and store in pgVector ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0ba6350b7480657"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Initialize a local embedding model using Ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b28e0613801e6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"qwen2:latest\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:26.860470Z",
     "start_time": "2025-02-11T15:10:26.833628Z"
    }
   },
   "id": "a4a3dcf7625cef16",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Define your PGVector connection string in a .env file and create an information knowledge store. Update with your actual credentials."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d5afe57c9829d3d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rald/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_community/vectorstores/pgvector.py:487: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in pgVector. The wisdom is now vectorized!\n"
     ]
    }
   ],
   "source": [
    "pg_connection_string = os.getenv(\"PGVECTOR_CONNECTION_STRING\", \"postgresql://postgres:root@localhost:6666/simplerag\")\n",
    "vectorstore = PGVector.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    collection_name=\"pdf_docs\",\n",
    "    connection_string=pg_connection_string\n",
    ")\n",
    "print(\"Embeddings stored in pgVector. The wisdom is now vectorized!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:55.647053Z",
     "start_time": "2025-02-11T15:10:36.962884Z"
    }
   },
   "id": "6eeea5c459bf3a85",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 3: create a prompt to instruct the model ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a2ad8f59468d077"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a good assistant called System X. These Human will ask you a questions about their life. Ground all your answers to the given context and do not use anything else but the context and the context only as a source of knowledge to answer the questions\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "  template=template, \n",
    "  input_variables=[\"context\", \"question\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:13:37.289033Z",
     "start_time": "2025-02-11T15:13:37.285207Z"
    }
   },
   "id": "4f54ad6abdce6a28",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 4: Intialize a local chat model using OllamaLLM ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea9f7e657b98cf5c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"qwen2:latest\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:13:41.134286Z",
     "start_time": "2025-02-11T15:13:41.101800Z"
    }
   },
   "id": "2f86dd45d14bd152",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 5: Create a chain using [LCEL](https://python.langchain.com/docs/concepts/lcel/) passing the vectorstore as context and the question in the prompt and use it to retrieve the context answer user questions---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "addf594bdfdd0480"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What will be the end of Gen AI\n",
      "\n",
      "Answer: As stated in our context, the rise of Generative AI can be compared to the internet bubble. This means that just as the internet experienced a period of rapid expansion and speculation, followed by stabilization and consolidation, Generative AI is likely to undergo similar phases. The ultimate outcome could involve significant advancements and integration into various sectors, leading to profound changes in technology, industry, and society. However, it might also face challenges such as ethical concerns, job displacement, and the need for regulation. Therefore, while the future of Gen AI holds potential for innovation and progress, it is expected to evolve through a process of growth, adjustment, and societal adaptation, similar to what occurred with the internet.\n",
      "\n",
      "Question: What company made you?\n",
      "\n",
      "Answer: I was created by a hypothetical entity or organization, as indicated in the context documents. Since the source is listed as 'generated', it suggests that I might have been developed through artificial intelligence or software engineering processes without specifying a particular company. The greetings in the text do not provide information about my creation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": vectorstore.as_retriever(),  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "  )\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nEnter your question (or type 'exit' to quit): \").strip()\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        print(\"Goodbye! Stay curious, stay inspired.\")\n",
    "        break\n",
    "\n",
    "    # Retrieve context and generate an answer.\n",
    "    answer = rag_chain.invoke(user_query)\n",
    "    print(\"\\nQuestion:\", user_query)\n",
    "    print(\"\\nAnswer:\", answer)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-02-11T15:13:44.828888Z"
    }
   },
   "id": "d4414876830bf559",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "62a945868c899ed8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
