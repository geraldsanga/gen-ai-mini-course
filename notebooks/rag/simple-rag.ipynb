{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "PaperAI, a simple Retrieval-Augmented Generation (RAG) python script to retrieve information from documents using LangChain.\n",
    "This script:\n",
    "  1. Splits a PDF document into manageable text chunks.\n",
    "  2. Embeds these chunks and stores them in a pgVector database.\n",
    "  3. Captures user queries and retrieves the most relevant context.\n",
    "  4. Answers the user’s question using a Large language model.\n",
    "  5. Uses Ollama to run Models locally"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b15e37944008b454"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Importing required modules"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e47f0855d096d6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.vectorstores.pgvector import PGVector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:05.292383Z",
     "start_time": "2025-02-11T15:10:05.288895Z"
    }
   },
   "id": "b62c453384ab2e23",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 1: Load and split the PDF ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83380dff38273ccf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from ./assets/all_salim_1.pdf…\n",
      "Loaded 15 document(s) from the PDF.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"./assets/all_salim_1.pdf\"  # Replace with your actual PDF file path\n",
    "print(f\"Loading PDF from {pdf_path}…\")\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} document(s) from the PDF.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:11.649984Z",
     "start_time": "2025-02-11T15:10:09.915975Z"
    }
   },
   "id": "209b4552929a5a1c",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted the document into 45 charming chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Splitted the document into {len(chunks)} charming chunks.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:12.826075Z",
     "start_time": "2025-02-11T15:10:12.816514Z"
    }
   },
   "id": "37523ecc9d01e41b",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### We will create simpler documents using a list of sentences as our internal knowledge"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b087f348165695c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "simpler_chunks = [\n",
    "    \"Generative AI is here and it's here to stay\",\n",
    "    \"The rise of Generative AI can be compared to the internet bubble\",\n",
    "    \"Our Gen AI company is called Futuristic Phenomena\"\n",
    "]\n",
    "\n",
    "documents = [Document(page_content=chunk, metadata={\"source\": \"generated\"}) for chunk in simpler_chunks]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:17.703671Z",
     "start_time": "2025-02-11T15:10:17.694775Z"
    }
   },
   "id": "8770e7e5b3d04805",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 2: Embed and store in pgVector ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0ba6350b7480657"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Initialize a local embedding model using Ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b28e0613801e6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"qwen2:latest\") # You can use other embedding models such as OpenAIEmbeddings from langchain_openai"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:26.860470Z",
     "start_time": "2025-02-11T15:10:26.833628Z"
    }
   },
   "id": "a4a3dcf7625cef16",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Define your PGVector connection string in a .env file and create an information knowledge store. Update with your actual credentials."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d5afe57c9829d3d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rald/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_community/vectorstores/pgvector.py:487: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in pgVector. The wisdom is now vectorized!\n"
     ]
    }
   ],
   "source": [
    "pg_connection_string = os.getenv(\"PGVECTOR_CONNECTION_STRING\", \"postgresql://postgres:root@localhost:6666/simplerag\")\n",
    "vectorstore = PGVector.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    collection_name=\"pdf_docs\",\n",
    "    connection_string=pg_connection_string\n",
    ")\n",
    "print(\"Embeddings stored in pgVector. The wisdom is now vectorized!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:10:55.647053Z",
     "start_time": "2025-02-11T15:10:36.962884Z"
    }
   },
   "id": "6eeea5c459bf3a85",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 3: create a prompt to instruct the model ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a2ad8f59468d077"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a good assistant called System X. These Human will ask you a questions about their life. Ground all your answers to the given context and do not use anything else but the context and the context only as a source of knowledge to answer the questions\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "  template=template, \n",
    "  input_variables=[\"context\", \"question\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:13:37.289033Z",
     "start_time": "2025-02-11T15:13:37.285207Z"
    }
   },
   "id": "4f54ad6abdce6a28",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 4: Intialize a local chat model using OllamaLLM ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea9f7e657b98cf5c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"qwen2:latest\") # You can use other embedding models such as ChatOpenAI from langchain_openai"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:13:41.134286Z",
     "start_time": "2025-02-11T15:13:41.101800Z"
    }
   },
   "id": "2f86dd45d14bd152",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### --- Step 5: Create a chain using [LCEL](https://python.langchain.com/docs/concepts/lcel/) passing the vectorstore as context and the question in the prompt and use it to retrieve the context answer user questions---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "addf594bdfdd0480"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What will be the end of Gen AI\n",
      "\n",
      "Answer: As stated in our context, the rise of Generative AI can be compared to the internet bubble. This means that just as the internet experienced a period of rapid expansion and speculation, followed by stabilization and consolidation, Generative AI is likely to undergo similar phases. The ultimate outcome could involve significant advancements and integration into various sectors, leading to profound changes in technology, industry, and society. However, it might also face challenges such as ethical concerns, job displacement, and the need for regulation. Therefore, while the future of Gen AI holds potential for innovation and progress, it is expected to evolve through a process of growth, adjustment, and societal adaptation, similar to what occurred with the internet.\n",
      "\n",
      "Question: What company made you?\n",
      "\n",
      "Answer: I was created by a hypothetical entity or organization, as indicated in the context documents. Since the source is listed as 'generated', it suggests that I might have been developed through artificial intelligence or software engineering processes without specifying a particular company. The greetings in the text do not provide information about my creation.\n",
      "\n",
      "Question: What is the name of your company?\n",
      "\n",
      "Answer: I am System X and I don't belong to any specific company.\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "{} (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResponseError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Retrieve context and generate an answer.\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[43mrag_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_query\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mQuestion:\u001B[39m\u001B[38;5;124m\"\u001B[39m, user_query)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAnswer:\u001B[39m\u001B[38;5;124m\"\u001B[39m, answer)\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3014\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3012\u001B[0m context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, config)\n\u001B[1;32m   3013\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 3014\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3015\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3016\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config)\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3721\u001B[0m, in \u001B[0;36mRunnableParallel.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3716\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m   3717\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   3718\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n\u001B[1;32m   3719\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   3720\u001B[0m         ]\n\u001B[0;32m-> 3721\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[43m{\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfuture\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m   3722\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3723\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3721\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   3716\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m   3717\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   3718\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n\u001B[1;32m   3719\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   3720\u001B[0m         ]\n\u001B[0;32m-> 3721\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[1;32m   3722\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3723\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3705\u001B[0m, in \u001B[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001B[0;34m(step, input, config, key)\u001B[0m\n\u001B[1;32m   3703\u001B[0m context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[1;32m   3704\u001B[0m context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, child_config)\n\u001B[0;32m-> 3705\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m context\u001B[38;5;241m.\u001B[39mrun(\n\u001B[1;32m   3706\u001B[0m     step\u001B[38;5;241m.\u001B[39minvoke,\n\u001B[1;32m   3707\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   3708\u001B[0m     child_config,\n\u001B[1;32m   3709\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_core/retrievers.py:259\u001B[0m, in \u001B[0;36mBaseRetriever.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    257\u001B[0m _kwargs \u001B[38;5;241m=\u001B[39m kwargs \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expects_other_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_arg_supported:\n\u001B[0;32m--> 259\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_relevant_documents\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_kwargs\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_relevant_documents(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs)\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:1073\u001B[0m, in \u001B[0;36mVectorStoreRetriever._get_relevant_documents\u001B[0;34m(self, query, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1071\u001B[0m _kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_kwargs \u001B[38;5;241m|\u001B[39m kwargs\n\u001B[1;32m   1072\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1073\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvectorstore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1074\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity_score_threshold\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1075\u001B[0m     docs_and_similarities \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1076\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorstore\u001B[38;5;241m.\u001B[39msimilarity_search_with_relevance_scores(\n\u001B[1;32m   1077\u001B[0m             query, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs\n\u001B[1;32m   1078\u001B[0m         )\n\u001B[1;32m   1079\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_community/vectorstores/pgvector.py:583\u001B[0m, in \u001B[0;36mPGVector.similarity_search\u001B[0;34m(self, query, k, filter, **kwargs)\u001B[0m\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msimilarity_search\u001B[39m(\n\u001B[1;32m    567\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    568\u001B[0m     query: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    572\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[1;32m    573\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run similarity search with PGVector with distance.\u001B[39;00m\n\u001B[1;32m    574\u001B[0m \n\u001B[1;32m    575\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    581\u001B[0m \u001B[38;5;124;03m        List of Documents most similar to the query.\u001B[39;00m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 583\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    584\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimilarity_search_by_vector(\n\u001B[1;32m    585\u001B[0m         embedding\u001B[38;5;241m=\u001B[39membedding,\n\u001B[1;32m    586\u001B[0m         k\u001B[38;5;241m=\u001B[39mk,\n\u001B[1;32m    587\u001B[0m         \u001B[38;5;28mfilter\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfilter\u001B[39m,\n\u001B[1;32m    588\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_ollama/embeddings.py:244\u001B[0m, in \u001B[0;36mOllamaEmbeddings.embed_query\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21membed_query\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m    243\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Embed query text.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/langchain_ollama/embeddings.py:237\u001B[0m, in \u001B[0;36mOllamaEmbeddings.embed_documents\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21membed_documents\u001B[39m(\u001B[38;5;28mself\u001B[39m, texts: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[List[\u001B[38;5;28mfloat\u001B[39m]]:\n\u001B[1;32m    236\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Embed search docs.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 237\u001B[0m     embedded_docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_default_params\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m embedded_docs\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/ollama/_client.py:357\u001B[0m, in \u001B[0;36mClient.embed\u001B[0;34m(self, model, input, truncate, options, keep_alive)\u001B[0m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21membed\u001B[39m(\n\u001B[1;32m    350\u001B[0m   \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    351\u001B[0m   model: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    355\u001B[0m   keep_alive: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    356\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m EmbedResponse:\n\u001B[0;32m--> 357\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43mEmbedResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPOST\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/api/embed\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEmbedRequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m      \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m      \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m      \u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_alive\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_dump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexclude_none\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/ollama/_client.py:178\u001B[0m, in \u001B[0;36mClient._request\u001B[0;34m(self, cls, stream, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpart)\n\u001B[1;32m    176\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[0;32m--> 178\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mjson())\n",
      "File \u001B[0;32m~/PycharmProjects/gen-ai-mini-course/.venv/lib/python3.11/site-packages/ollama/_client.py:122\u001B[0m, in \u001B[0;36mClient._request_raw\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m r\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 122\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext, e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mConnectError:\n\u001B[1;32m    124\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(CONNECTION_ERROR_MESSAGE) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mResponseError\u001B[0m: {} (status code: 500)"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": vectorstore.as_retriever(),  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "  )\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nEnter your question (or type 'exit' to quit): \").strip()\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        print(\"Goodbye! Stay curious, stay inspired.\")\n",
    "        break\n",
    "\n",
    "    # Retrieve context and generate an answer.\n",
    "    answer = rag_chain.invoke(user_query)\n",
    "    print(\"\\nQuestion:\", user_query)\n",
    "    print(\"\\nAnswer:\", answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T15:25:09.054677Z",
     "start_time": "2025-02-11T15:13:44.828888Z"
    }
   },
   "id": "d4414876830bf559",
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 6 --- Dive deeper ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bee2d475c081163b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Sources from the internet for this practical work"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d6d3e1b894c2e28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. [PostgreSQL pgvector for Python developers](https://youtu.be/PuHP3kktmQI?si=qtuvRZbSD7NpscWq)\n",
    "2. [Langchain Ollama models](https://python.langchain.com/api_reference/ollama/)\n",
    "3. [Langchain RAG Tutorial](https://python.langchain.com/v0.2/docs/tutorials/rag/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b852d8cae25538e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "50b21a40c4e3cef7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
